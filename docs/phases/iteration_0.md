# Iteration 0: Project & Infrastructure Bootstrap

## Overview

This phase establishes the foundational infrastructure for our autonomous AI development team. We'll set up the repository structure, cloud resources, local development environment, and CI/CD pipelines that will support all subsequent iterations.

## Why This Phase Matters

The infrastructure choices made during this phase will impact every aspect of the system's operation. By properly configuring PostgreSQL with the complete schema design from our blueprint, we ensure all agent activities can be logged and queried. Setting up Redis provides the task queue backbone required for asynchronous agent operations. The repository structure and CI pipeline establish the foundation for the AI team's code management and quality control processes.

## Expected Outcomes

After completing this phase, we will have:

1. A fully functional GitHub/GitLab repository with the appropriate directory structure
2. Cloud infrastructure (PostgreSQL, Redis) provisioned and configured
3. Local development environment established with Docker Compose
4. Initial CI pipeline for automated testing
5. Documentation of system schemas and conventions

## Implementation Tasks

### Task 1: Repository Structure Setup

**What needs to be done:**
Create a mono-repo on GitHub/GitLab with the following directory structure:

- `/agents` - All agent code and core implementation
- `/infra` - Infrastructure configuration (Terraform, Docker, etc.)
- `/dashboard` - The web-based monitoring dashboard
- `/docs` - Documentation and reference materials
- `/app` - The application code that will be generated by the AI team

**Why this task is necessary:**
This structure separates concerns while maintaining all code in a single repository, allowing agents to commit to different parts of the codebase seamlessly.

**Files to be created:**

- `.gitignore` - Configured for Python, Node.js, and environment files
- `README.md` - Project overview and setup instructions
- `.github/workflows/` - Directory for GitHub Actions configurations

**Implementation guidelines:**

```bash
# Clone the repository locally
git clone <repository-url>
cd <repository-name>

# Create directory structure
mkdir -p agents infra dashboard docs app .github/workflows

# Create initial README.md
cat > README.md << 'EOF'
# Autonomous AI Development Team

This project implements a fully autonomous AI-powered software development team capable of planning, coding, testing, and deploying applications with minimal human intervention.

## Repository Structure

- `/agents` - Core agent implementation and orchestration
- `/infra` - Infrastructure configuration
- `/dashboard` - Monitoring and control dashboard
- `/docs` - Documentation and specifications
- `/app` - Application code generated by AI agents

## Getting Started

See [docs/setup.md](docs/setup.md) for installation and configuration instructions.
EOF

# Create .gitignore
cat > .gitignore << 'EOF'
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
env/
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
*.egg-info/
.installed.cfg
*.egg
.env
.venv
venv/
ENV/

# Node.js
node_modules/
npm-debug.log
yarn-debug.log
yarn-error.log
.next/
out/
.nuxt/
.cache/

# Docker
docker-compose.override.yml

# Local development
.env.local
.env.development.local
.env.test.local
.env.production.local

# IDE
.idea/
.vscode/
*.swp
*.swo
EOF

# Initial commit
git add .
git commit -m "Initial repository structure"
git push origin main
```

### Task 2: PostgreSQL Schema Configuration

**What needs to be done:**
Create and configure the PostgreSQL database schema according to the blueprint specifications, including all tables for agent metadata, messages, activities, artifacts, tasks, and meetings.

**Why this task is necessary:**
The database schema is the foundation of our shared memory system, enabling comprehensive logging and communication between agents.

**Files to be created:**

- `infra/database/schema.sql` - SQL schema definition
- `infra/database/migrations/0001_initial.sql` - Initial migration script

**Implementation guidelines:**

```sql
-- infra/database/schema.sql

-- Enable required extensions
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
CREATE EXTENSION IF NOT EXISTS "pgvector";

-- Agent Metadata
CREATE TABLE agents (
    agent_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    agent_type VARCHAR(50) NOT NULL, -- PM, ScrumMaster, FrontendLead, etc.
    agent_name VARCHAR(100) NOT NULL,
    created_at TIMESTAMP NOT NULL DEFAULT NOW(),
    capabilities JSONB, -- Specialized capabilities based on role
    active BOOLEAN NOT NULL DEFAULT TRUE
);

-- Inter-Agent Messages
CREATE TABLE agent_messages (
    message_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    timestamp TIMESTAMP NOT NULL DEFAULT NOW(),
    sender_id UUID REFERENCES agents(agent_id),
    receiver_id UUID REFERENCES agents(agent_id),
    message_type VARCHAR(50) NOT NULL, -- REQUEST, INFORM, PROPOSE, etc.
    content TEXT NOT NULL,
    related_task_id UUID, -- Optional reference to task
    metadata JSONB, -- Additional structured data
    parent_message_id UUID REFERENCES agent_messages(message_id), -- For threading
    context_vector VECTOR(1536) -- For semantic search of message content
);

-- Agent Activities and Decisions
CREATE TABLE agent_activities (
    activity_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    agent_id UUID REFERENCES agents(agent_id),
    timestamp TIMESTAMP NOT NULL DEFAULT NOW(),
    activity_type VARCHAR(100) NOT NULL, -- CodeGeneration, Review, Testing, etc.
    description TEXT NOT NULL,
    thought_process TEXT, -- Internal reasoning of agent
    input_data JSONB, -- What the agent worked with
    output_data JSONB, -- What the agent produced
    related_files TEXT[], -- File paths affected
    decisions_made JSONB, -- Structured record of decisions
    execution_time_ms INTEGER -- Performance tracking
);

-- Project Artifacts
CREATE TABLE artifacts (
    artifact_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    artifact_type VARCHAR(50) NOT NULL, -- Requirement, UserStory, Design, Code, Test, etc.
    title VARCHAR(255) NOT NULL,
    content TEXT,
    created_at TIMESTAMP NOT NULL DEFAULT NOW(),
    created_by UUID REFERENCES agents(agent_id),
    last_modified_at TIMESTAMP,
    last_modified_by UUID REFERENCES agents(agent_id),
    parent_id UUID REFERENCES artifacts(artifact_id), -- For hierarchical artifacts
    status VARCHAR(50) NOT NULL,
    metadata JSONB, -- Additional properties
    version INTEGER NOT NULL DEFAULT 1,
    content_vector VECTOR(1536) -- For semantic search
);

-- Tasks and Assignments
CREATE TABLE tasks (
    task_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    title VARCHAR(255) NOT NULL,
    description TEXT NOT NULL,
    created_at TIMESTAMP NOT NULL DEFAULT NOW(),
    created_by UUID REFERENCES agents(agent_id),
    assigned_to UUID REFERENCES agents(agent_id),
    sprint_id UUID, -- Reference to sprint
    priority INTEGER NOT NULL,
    status VARCHAR(50) NOT NULL, -- Backlog, InProgress, Review, Done, etc.
    estimated_effort FLOAT,
    actual_effort FLOAT,
    dependencies UUID[], -- Array of tasks this depends on
    metadata JSONB, -- Additional task data
    related_artifacts UUID[] -- References to requirements, designs, etc.
);

-- Meetings and Conversations
CREATE TABLE meetings (
    meeting_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    meeting_type VARCHAR(50) NOT NULL, -- Planning, StandUp, Review, Retrospective, Brainstorming
    start_time TIMESTAMP NOT NULL DEFAULT NOW(),
    end_time TIMESTAMP,
    participants UUID[] REFERENCES agents(agent_id),
    summary TEXT,
    decisions JSONB,
    action_items JSONB
);

CREATE TABLE meeting_conversations (
    conversation_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    meeting_id UUID REFERENCES meetings(meeting_id),
    sequence_number INTEGER NOT NULL, -- Order in conversation
    timestamp TIMESTAMP NOT NULL DEFAULT NOW(),
    speaker_id UUID REFERENCES agents(agent_id),
    message TEXT NOT NULL,
    message_type VARCHAR(50), -- Question, Answer, Proposal, etc.
    context JSONB -- Reference to what is being discussed
);

-- Create indexes for performance
CREATE INDEX idx_agent_messages_sender ON agent_messages(sender_id);
CREATE INDEX idx_agent_messages_receiver ON agent_messages(receiver_id);
CREATE INDEX idx_agent_activities_agent ON agent_activities(agent_id);
CREATE INDEX idx_tasks_assigned_to ON tasks(assigned_to);
CREATE INDEX idx_tasks_status ON tasks(status);
CREATE INDEX idx_artifacts_type ON artifacts(artifact_type);
CREATE INDEX idx_artifacts_creator ON artifacts(created_by);
```

### Task 3: Cloud Infrastructure Setup

**What needs to be done:**
Set up the required cloud resources on Google Cloud Platform (GCP), including PostgreSQL, Redis, and Cloud Storage.

**Why this task is necessary:**
Cloud resources provide the persistent infrastructure needed for our agent system to operate reliably.

**Files to be created:**

- `infra/terraform/main.tf` - Terraform configuration for cloud resources
- `infra/terraform/variables.tf` - Variables for the Terraform configuration
- `infra/terraform/outputs.tf` - Outputs from the Terraform configuration

**Implementation guidelines:**

```hcl
# infra/terraform/main.tf

provider "google" {
  project = var.project_id
  region  = var.region
  zone    = var.zone
}

# Cloud SQL (PostgreSQL)
resource "google_sql_database_instance" "postgres" {
  name             = "agent-team-postgres"
  database_version = "POSTGRES_14"
  region           = var.region

  settings {
    tier = var.db_tier

    database_flags {
      name  = "cloudsql.enable_pgvector"
      value = "on"
    }

    backup_configuration {
      enabled = true
      point_in_time_recovery_enabled = true
    }
  }

  deletion_protection = false
}

resource "google_sql_database" "agent_db" {
  name     = "agent_team"
  instance = google_sql_database_instance.postgres.name
}

resource "google_sql_user" "agent_user" {
  name     = "agent_user"
  instance = google_sql_database_instance.postgres.name
  password = var.db_password
}

# Redis (Memorystore)
resource "google_redis_instance" "agent_redis" {
  name           = "agent-team-redis"
  tier           = "BASIC"
  memory_size_gb = 1
  region         = var.region

  authorized_network = google_compute_network.vpc_network.id
}

# VPC Network for Redis
resource "google_compute_network" "vpc_network" {
  name = "agent-team-network"
}

# Cloud Storage Bucket
resource "google_storage_bucket" "artifact_storage" {
  name     = "${var.project_id}-artifacts"
  location = var.region
  uniform_bucket_level_access = true
}

# infra/terraform/variables.tf

variable "project_id" {
  description = "The GCP project ID"
  type        = string
}

variable "region" {
  description = "The GCP region to deploy resources"
  type        = string
  default     = "us-central1"
}

variable "zone" {
  description = "The GCP zone within the region"
  type        = string
  default     = "us-central1-a"
}

variable "db_tier" {
  description = "The database machine tier"
  type        = string
  default     = "db-f1-micro"
}

variable "db_password" {
  description = "The database password"
  type        = string
  sensitive   = true
}

# infra/terraform/outputs.tf

output "database_connection_name" {
  value = google_sql_database_instance.postgres.connection_name
}

output "redis_host" {
  value = google_redis_instance.agent_redis.host
}

output "redis_port" {
  value = google_redis_instance.agent_redis.port
}

output "storage_bucket" {
  value = google_storage_bucket.artifact_storage.name
}
```

### Task 4: Docker Local Development Environment

**What needs to be done:**
Create a Docker Compose configuration for local development that includes PostgreSQL, Redis, and containers for Python and Node.js development.

**Why this task is necessary:**
A consistent local development environment ensures all team members can work with identical configurations and simplifies testing.

**Files to be created:**

- `docker-compose.yml` - Docker Compose configuration
- `infra/docker/postgres/init.sql` - Database initialization script
- `infra/docker/python/Dockerfile` - Python development container
- `infra/docker/node/Dockerfile` - Node.js development container

**Implementation guidelines:**

```yaml
# docker-compose.yml

version: "3.8"

services:
  postgres:
    image: ankane/pgvector:latest
    ports:
      - "5432:5432"
    environment:
      POSTGRES_DB: agent_team
      POSTGRES_USER: agent_user
      POSTGRES_PASSWORD: agent_password
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./infra/docker/postgres/init.sql:/docker-entrypoint-initdb.d/init.sql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U agent_user -d agent_team"]
      interval: 5s
      timeout: 5s
      retries: 5

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 5s
      retries: 5

  python:
    build:
      context: .
      dockerfile: infra/docker/python/Dockerfile
    volumes:
      - .:/app
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      - DATABASE_URL=postgresql://agent_user:agent_password@postgres:5432/agent_team
      - REDIS_URL=redis://redis:6379/0
    command: sleep infinity

  node:
    build:
      context: .
      dockerfile: infra/docker/node/Dockerfile
    volumes:
      - .:/app
    ports:
      - "3000:3000"
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      - DATABASE_URL=postgresql://agent_user:agent_password@postgres:5432/agent_team
    command: sleep infinity

volumes:
  postgres_data:
  redis_data:
```

```dockerfile
# infra/docker/python/Dockerfile

FROM python:3.12-slim

WORKDIR /app

RUN apt-get update && apt-get install -y \
    git \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

RUN pip install --no-cache-dir \
    fastapi==0.110.0 \
    uvicorn==0.27.1 \
    sqlalchemy==2.0.28 \
    asyncpg==0.29.0 \
    pgvector==0.2.4 \
    celery==5.3.6 \
    redis==5.0.1 \
    psycopg2-binary==2.9.9 \
    pydantic==2.5.3 \
    pygit2==1.13.3

ENV PYTHONPATH=/app
```

```dockerfile
# infra/docker/node/Dockerfile

FROM node:20-alpine

WORKDIR /app

RUN apk add --no-cache git

RUN npm install -g \
    next@14 \
    typescript@5.5 \
    eslint@8 \
    prettier@3

ENV NODE_PATH=/app/node_modules
ENV PATH=/app/node_modules/.bin:$PATH
```

```sql
# infra/docker/postgres/init.sql

-- Initialize database with schema
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
CREATE EXTENSION IF NOT EXISTS "pgvector";

-- Copy the schema.sql content here
-- ...
```

### Task 5: CI Pipeline Configuration

**What needs to be done:**
Set up GitHub Actions for continuous integration, including automated testing with pytest.

**Why this task is necessary:**
Automated testing is essential for maintaining code quality and will be used by the QA agent to verify code changes.

**Files to be created:**

- `.github/workflows/ci.yml` - GitHub Actions workflow configuration
- `pytest.ini` - Configuration for pytest
- `agents/tests/__init__.py` - Empty file to mark tests directory

**Implementation guidelines:**

```yaml
# .github/workflows/ci.yml

name: CI Pipeline

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest

    services:
      postgres:
        image: ankane/pgvector:latest
        env:
          POSTGRES_DB: agent_team_test
          POSTGRES_USER: agent_user
          POSTGRES_PASSWORD: agent_password
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.12"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-cov
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

      - name: Initialize test database
        run: |
          psql -h localhost -U agent_user -d agent_team_test -f infra/database/schema.sql
        env:
          PGPASSWORD: agent_password

      - name: Run tests
        run: |
          pytest --cov=agents
        env:
          DATABASE_URL: postgresql://agent_user:agent_password@localhost:5432/agent_team_test
          REDIS_URL: redis://localhost:6379/0
```

```ini
# pytest.ini

[pytest]
testpaths = agents/tests
python_files = test_*.py
python_functions = test_*
```

### Task 6: Documentation Setup

**What needs to be done:**
Create initial documentation for the system, including schema definitions, setup instructions, and development conventions.

**Why this task is necessary:**
Clear documentation ensures all team members understand the system's design and can contribute effectively.

**Files to be created:**

- `docs/setup.md` - Setup and installation instructions
- `docs/schema.md` - Database schema documentation
- `docs/conventions.md` - Coding and development conventions

**Implementation guidelines:**

````markdown
# docs/setup.md

# Setup Instructions

This document describes how to set up the Autonomous AI Development Team project for local development.

## Prerequisites

- Docker and Docker Compose
- Git
- Python 3.12 (for local development outside Docker)
- Node.js 20 (for local development outside Docker)

## Local Development Setup

1. Clone the repository:
   ```bash
   git clone <repository-url>
   cd <repository-name>
   ```
````

2. Start the Docker Compose environment:

   ```bash
   docker-compose up -d
   ```

3. Initialize the database:

   ```bash
   docker-compose exec postgres psql -U agent_user -d agent_team -f /docker-entrypoint-initdb.d/init.sql
   ```

4. Access the Python development container:

   ```bash
   docker-compose exec python bash
   ```

5. Access the Node.js development container:
   ```bash
   docker-compose exec node sh
   ```

## Cloud Deployment

See `docs/cloud_setup.md` for instructions on deploying to Google Cloud Platform.

````

```markdown
# docs/schema.md

# Database Schema

This document describes the database schema used by the Autonomous AI Development Team.

## Overview

Our system uses PostgreSQL with the pgvector extension for storing and retrieving all agent data, including:

- Agent metadata and capabilities
- Inter-agent messages
- Agent activities and decisions
- Project artifacts
- Tasks and assignments
- Meeting records and conversations

## Tables

### agents

Stores information about each agent in the system.

| Column | Type | Description |
|--------|------|-------------|
| agent_id | UUID | Primary key |
| agent_type | VARCHAR(50) | Role of the agent (PM, ScrumMaster, etc.) |
| agent_name | VARCHAR(100) | Name of the agent |
| created_at | TIMESTAMP | When the agent was created |
| capabilities | JSONB | Agent-specific capabilities |
| active | BOOLEAN | Whether the agent is active |

### agent_messages

Records all messages sent between agents.

| Column | Type | Description |
|--------|------|-------------|
| message_id | UUID | Primary key |
| timestamp | TIMESTAMP | When the message was sent |
| sender_id | UUID | ID of the sending agent |
| receiver_id | UUID | ID of the receiving agent |
| message_type | VARCHAR(50) | Type of message (REQUEST, INFORM, etc.) |
| content | TEXT | Content of the message |
| related_task_id | UUID | Optional task reference |
| metadata | JSONB | Additional structured data |
| parent_message_id | UUID | For threaded conversations |
| context_vector | VECTOR(1536) | Vector embedding for semantic search |

... [Additional tables would be documented similarly]
````

```markdown
# docs/conventions.md

# Development Conventions

This document outlines the coding and development conventions for the Autonomous AI Development Team project.

## Code Style

### Python

- Follow PEP 8 style guide
- Use Black for formatting
- Maximum line length: 88 characters
- Use type hints for all function parameters and return values
- Use docstrings for all classes and functions

### JavaScript/TypeScript

- Follow Airbnb JavaScript Style Guide
- Use Prettier for formatting
- Use ESLint for linting
- Use TypeScript interfaces for all data structures
- Use JSDoc comments for all functions

## Git Workflow

- Use feature branches for all changes
- Branch naming: `feature/[feature-name]` or `fix/[issue-name]`
- Commit messages: Follow Conventional Commits format
  - `feat: add new feature`
  - `fix: resolve bug in feature`
  - `docs: update documentation`
  - `chore: update dependencies`
- Create pull requests for all changes
- Required PR approvals: 1

## Testing

- All code should have unit tests
- Minimum test coverage: 80%
- Integration tests for critical paths
- Test naming: `test_[what_is_being_tested].py`

## Documentation

- All public APIs must be documented
- Update documentation with code changes
- Use Markdown for documentation
- Keep documentation in the `docs/` directory
```

## Post-Implementation Verification

After completing all tasks, verify the setup by:

1. Starting the Docker Compose environment
2. Connecting to the PostgreSQL database and verifying tables were created correctly
3. Running a simple test through the CI pipeline
4. Checking that documentation is accurate and comprehensive

All infrastructure should be fully operational and ready for the next phase of development.
